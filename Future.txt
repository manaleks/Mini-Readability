О перспективах, идеях улучшения и будущем MiniReadability.

Разработка программы шла в течение трёх недель. Так получилось, что до этого момента я не связывался с питонами, потому изучения языка происходило параллельно с написанием самой программы.
Исходя из этого я очень допускаю, что использованные методы и решения могут быть не совсем/совсем не идеальными.
Потому 2 этап жизни MiniReadability может быть только оптимизацией, поиском понастоящему универсальных, экономичных и рациональных решений. 
Когда же на данном фронте всё будет спокойно, и критические ошибки/вылеты/вставления кусков HTML будут исключены, можно приступать к облагораживанию программы:
- записывать не просто .txt файл, а позволять пользователю выбирать: .doc, .docx, .pdf, .txt...
- добавлять картинки, приблизительно с тем же расположением, что и на сайте. Или оставлять ссылку с обозначением, что тут должна быть картинка(в случае с .txt)
- упростить запуск: URL должен отправляться в командную строку вместе с исполняемым модулем сразу, а не после отклика программы.
Это основные и довольно очевидные усовершенствования, если поразмыслить шире, то можно раскачать программу до огромных масштабов.

Дальше идёт чистая фантазия. Читать только если интересно и нет других дел:)
Первое предназначение программы – добыть информацию из интернета, без лишнего мусора.
Но бывает так (очень часто бывает), что в самом тексте содержится "мусор". Качество многих статей сильно хромает, а некоторые впрямую дезинформируют людей. Перекапывать их все и оценивать вручную не очень по-айтишному, потому нужно сделать встроенную оценку добытого текста.
До этого программа работала лишь с визуальным оформлением. Теперь же она возьмётся за синтаксис и семантику текста.
Как это работает?
У любой темы есть слова индикаторы, по которым можно оценить степень соответствия текста и даже его адекватность и качество. Например, если статья содержит большое количество слов вроде: истина, поток сознания, исцеление, телепатия и т.п.. То с большой степенью вероятности можно сказать, что эта статься слабо относится к науке.
Считая использованные слова и соотнося эти данные с общим количеством слов можно получать приблизительное понимание о содержании.
А соотношение количества использованных слов к общему их чилу в тексте может быть критерием разнообразия речи автора, что может быть полезно, например филологам в их изысканиях.
В итоге при создании текстового файла пользователь сможет получить и краткую оценку того, что собирается прочитать по тем параметрам, которые его интересуют.
Затем эти данные можно собирать, надстраивать целую базу данных о сайтах, что снова будет помогать в их оценке. Призывать экспертов предметных областей, которые более верно смогут указать на признаки соответствия текстов теме и критерии оценки. А также необходимо будет постоянно улучшать методы оценки и подключать нейронные сети. Ведь всё должно быть по-айтишному.
При большой базе данных, пользователь сможет искать ключевые слова, интересующей его темы и выставлять свои критерии отбора. Так он сможет получить именно ту информацию, которую ищет в обработанном, красивом виде.
+ простая монетизация такого проекта. Заплатившим пользователям предоставляется более широкий список критериев отбора.
Да... Это уже больше похоже на поисковую систему...
